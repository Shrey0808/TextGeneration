{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "sOztH88kR7JC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiKntwk3pot-",
        "outputId": "6811a1f4-d85b-4bc5-f076-72c40560b1dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the RNN model\n",
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
        "        super(CharRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input)\n",
        "        output, hidden = self.lstm(embedded, hidden)\n",
        "        output = self.fc(output[:, -1, :])\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device),\n",
        "                torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device))"
      ],
      "metadata": {
        "id": "2L6X0IowR-bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the input text\n",
        "def preprocess(text):\n",
        "    chars = list(set(text))\n",
        "    char_to_index = {char: i for i, char in enumerate(chars)}\n",
        "    index_to_char = {i: char for i, char in enumerate(chars)}\n",
        "    return chars, char_to_index, index_to_char"
      ],
      "metadata": {
        "id": "HXHdZDlVR_SV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate training data\n",
        "def generate_training_data(text, seq_length):\n",
        "    input_seqs = []\n",
        "    target_seqs = []\n",
        "    for i in range(len(text) - seq_length):\n",
        "        input_seq = text[i:i+seq_length]\n",
        "        target_seq = text[i+seq_length]\n",
        "        input_seqs.append([char_to_index[char] for char in input_seq])\n",
        "        target_seqs.append(char_to_index[target_seq])\n",
        "    return input_seqs, target_seqs"
      ],
      "metadata": {
        "id": "YD4-rBMhSBj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "seq_length = 100\n",
        "hidden_size = 256\n",
        "num_layers = 2\n",
        "learning_rate = 0.001\n",
        "num_epochs = 50\n",
        "batch_size = 64"
      ],
      "metadata": {
        "id": "gyG_HVqrSICH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the input text\n",
        "filename = \"Deep Learning.txt\"\n",
        "text = open(filename, 'r', encoding='utf-8').read()\n",
        "chars, char_to_index, index_to_char = preprocess(text)\n",
        "input_seqs, target_seqs = generate_training_data(text, seq_length)"
      ],
      "metadata": {
        "id": "AhcYEuFFSLed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to PyTorch tensors\n",
        "input_seqs = torch.tensor(input_seqs, dtype=torch.long).to(device)\n",
        "target_seqs = torch.tensor(target_seqs, dtype=torch.long).to(device)\n",
        "\n",
        "# Define input and output sizes based on the processed text\n",
        "input_size = len(chars)\n",
        "output_size = len(chars)"
      ],
      "metadata": {
        "id": "_W4ZkfxySPn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model = CharRNN(input_size, hidden_size, output_size, num_layers).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "hd0JAosUSTr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "total_batches = len(input_seqs) // batch_size\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "\n",
        "    # Shuffle the data\n",
        "    indices = torch.randperm(len(input_seqs))\n",
        "    input_seqs = input_seqs[indices]\n",
        "    target_seqs = target_seqs[indices]\n",
        "\n",
        "    # Mini-batch training\n",
        "    for batch in range(total_batches):\n",
        "        start_idx = batch * batch_size\n",
        "        end_idx = start_idx + batch_size\n",
        "        input_batch = input_seqs[start_idx:end_idx]\n",
        "        target_batch = target_seqs[start_idx:end_idx]\n",
        "\n",
        "        hidden = model.init_hidden(input_batch.size(0))\n",
        "        hidden = tuple(tensor.to(device) for tensor in hidden)\n",
        "\n",
        "        # Forward pass\n",
        "        output, hidden = model(input_batch, hidden)\n",
        "        loss = criterion(output, target_batch)\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Print the loss every 5 epochs till 50\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        average_loss = running_loss / total_batches\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0oHWwmnK_Tp",
        "outputId": "e292a738-a4c0-4e53-e573-36598fea89ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/50], Loss: 1.0563\n",
            "Epoch [10/50], Loss: 0.6268\n",
            "Epoch [15/50], Loss: 0.3806\n",
            "Epoch [20/50], Loss: 0.2395\n",
            "Epoch [25/50], Loss: 0.1681\n",
            "Epoch [30/50], Loss: 0.1179\n",
            "Epoch [35/50], Loss: 0.1075\n",
            "Epoch [40/50], Loss: 0.0783\n",
            "Epoch [45/50], Loss: 0.0635\n",
            "Epoch [50/50], Loss: 0.0807\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "sentence_end_markers = ['.', '?']  # Add other sentence end markers if needed\n",
        "end_indices = [i for i, char in enumerate(text) if char in sentence_end_markers]\n",
        "start = np.random.choice(end_indices) + 1  # Add 1 to start at the next character\n",
        "start_seq = text[start:start+seq_length]\n",
        "generated_text = start_seq\n",
        "\n",
        "with torch.no_grad():\n",
        "    hidden = model.init_hidden(1)\n",
        "    hidden = tuple(tensor.to(device) for tensor in hidden)\n",
        "\n",
        "    input_seq = torch.tensor([char_to_index[char] for char in start_seq], dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    num_words = len(generated_text.split())\n",
        "\n",
        "    while num_words < 100 or not generated_text.endswith((\".\", \"!\", \"?\")):\n",
        "        output, hidden = model(input_seq, hidden)\n",
        "        probabilities = nn.functional.softmax(output, dim=1).squeeze().cpu().numpy()\n",
        "        predicted_index = np.random.choice(len(chars), p=probabilities)\n",
        "        predicted_char = index_to_char[predicted_index]\n",
        "        generated_text += predicted_char\n",
        "        input_seq = torch.tensor([[predicted_index]], dtype=torch.long).to(device)\n",
        "\n",
        "        # Update the word count\n",
        "        num_words = len(generated_text.split())\n",
        "\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gx6p9QW_SYoK",
        "outputId": "f7cf46bd-e378-4e6d-dd7c-3f3c3ad139ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            " While supervised learning, which relies on labeled data, has been the dominant paradigm, unsupervised learning to improve the efficiency, generalization, and enhance its practical impact of Deep Learning. As Deep Learning models become increas and interdiscial institutions of the training and inference provide accurate natural language processing, healthcare, finance, robotics, and beyond. inding the development of explainable and interpretable Deep Learning models. As Deep Learning models become increasingly integrated intous in virtual assistants, voice neural network architectures, Deep Learning models can substundible data to predict market trends, detect anomalies, and other fields such as reinformation.\n",
            "\n",
            "In natural language processes.\n",
            "\n",
            "To address these pfrom complex market data, assisting to interdisciplina, and interact with objects, and perform complex tasks with precision and adaptability, making significant strides in areas like industrial automation, warehouse patterns, these potential to reshape industries, drive innovation, and interactive experiences.\n"
          ]
        }
      ]
    }
  ]
}